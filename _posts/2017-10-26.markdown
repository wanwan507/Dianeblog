---
layout:     post
title:      "阅读alpha zero笔记"
date:       2017-10-30 12:00:00
author:     "Diane"
header-img: "img/post-alpha-zero.jpg"
tags:
    - 增强学习
    - 深度学习
---

##阅读alpha zero笔记

####什么是alpha zero

最近，AlphaGo成为围棋中的击败世界冠军的第一个程序。在评价与选择alphago位置移动使用深度神经网络树搜索。这些神经网络是通过专家学习的有监督学习和自我学习强化学习训练而成的。在这里，我们介绍了一个完全基于强化学习的算法，没有人类数据，指导，或超越游戏规则的领域知识。alphago成为自己的老师：训练一个神经网络来预测AlphaGo的行动选择和AlphaGo比赛的胜者。该神经网络提高了树搜索的力量，重新形成更高质量的移动选择，在下一次迭代中更强的自我发挥。我们的新项目alphago零取得了超人的业绩，赢得对先前公布的100-0，将alphago冠军击败。

####序

人工智能已取得很大进展，使用受过训练的专家系统来复制人类专家的决策。然而，专家数据通常是昂贵的、不可靠的，或者根本不可用。即使有可靠的数据，它也可能对以这种方式培训的系统的性能造成限制。相反，强化学习系统是从他们自己的经验中训练出来的，原则上允许他们超越人的能力，并在缺乏专业知识的领域运作。

发布的版本，我们称之为AlphaGo Fan，2015年10月击败欧洲冠军范辉。alphago利用两深层神经网络：策略网络的输出转移概率，和价值网络的输出位置的评价。策略网络最初是通过有监督的学习来精确预测人类专家的移动，随后通过政策梯度强化学习加以改进。价值网被训练以预测政策网络对自身的博弈赢家。一旦经过训练，这些网络结合蒙特卡洛树搜索（MCTS）提供前向搜索，运用政策网络来缩小搜索的高概率的动作，并利用价值网的工作（在蒙特卡洛推广使用快速推出政策一起）评价树中的位置。

它只由自我播放强化学习训练，从随机播放开始，没有任何监督或使用人的数据。其次，它只使用棋盘上的黑白石头作为输入特征。第三，它使用单一的神经网络，而不是单独的政策和价值网络。最后，它使用了一个简单的搜索树，依靠这种单一的神经网络进行位置和样品时，不执行任何Monte-Carlo展示。为了实现这些结果，我们引入一个新的增强算法，采用前向搜索里面的训练循环学习，从而迅速提高、精确和稳定的学习。

####alpha zero中的强化学习

新方法使用深度神经网络参数Fθ（θ）。该神经网络作为输入，其历史地位的原始表示，移动和输出概率，和一个值（p，v）= Fθ（S）。转移概率向量代表选择每一步的概率（包括通过），PA = PR(a|S）。v值是一个标量估计，估计当前玩家从位置S中获胜的概率。这个神经网络将策略网络和价值网络的角色结合成一个单一的体系结构。

在alphago zero 的神经网络是由一种新型的强化学习算法训练自我发挥的游戏。在每一个位置，一个能执行搜索，通过神经网络θ引导F。MCTS搜索输出概率π玩的每一个动作。这些搜索概率通常选择更强的动作比原移动概率神经网络的Fθ（S）；MCTS可能因此被看作是一个强大的政策改进算子。采用改进的基于策略的选择MCTS每个移动搜索，然后用游戏的赢家Z为价值–样品可以被看作是一个强有力的策略评价算子。强化学习算法的主要思想是在策略迭代程序重复使用搜索操作符：神经网络的参数进行更新，使转移概率值（p，v）= Fθ（S）更密切地配合改进的搜索概率和自我发挥的赢家（π，Z）；这些使用新的参数在自我的下一次迭代的发挥使搜索更强大。图1展示了自播放训练流水线。

<img src="/img/alphazero1.jpg" />

蒙特卡洛树搜索使用神经网络Fθ指导其模拟（见图2）。搜索树中的每个边（s，a）存储一个先验概率p（s，a），访问计数n（s，a），和一个动作值q（s，a）。每个模拟从根的状态和反复选择的动作，最大限度的置信上界Q（s，a）+ U（S，A），其中U（S，A）∝P（S，A）/（1 + N（S，A）），直到一个叶节点 s' 是遇到。这叶位扩展和评估只是一次由网络产生的先验概率和评估，（P（S'，·）、V（S'））= Fθ（S'）。
每个边缘（S，A）经过模拟更新来增加它的访问数N（S，A），并更新它的平均评价在这些模拟交易价值，Q（s，a）= 1 / N（S，A）s'| S，a→s'V（S'），其中S，a→s'表明模拟最终达成s'服用后移动一个位置S.

<img src="/img/alphazero2.jpg" />

MCTS可以被看作是一个自我发挥的算法，给出了神经网络参数θ和根的位置，计算搜索概率，π=αθ（S），与访问动作个数的幂成正比，π一∝N（S，A）1 /τ，其中τ是温度参数。

神经网络是由一个自我发挥的强化学习算法，采用能发挥每个移动训练。首先，神经网络权值初始化随机θ0。在每次重复i≥1，自玩游戏产生了（图1A）。在每一个时间步t，一个MCTS搜索πT =αθi−1（ST）是利用神经网络的Fθi−1以前的迭代执行，和移动是通过采样的搜索概率πT.游戏终止于步当双方都被pass，当搜索值低于辞职的门槛，或当游戏超过最大长度；游戏则拿给最终的奖赏rt∈{-1，1 } 。每个时间步t的数据存储为（ST，πT，ZT），ZT =±RT是游戏赢家从当前玩家的视角在步骤T.于此平行的参见（图1b），新的网络参数θi训练数据（S，π，Z）均匀采样的最后一次迭代步骤中的所有时间（S）自我发挥。神经网络（p，v）= Fθi（s）是调整为最小预测值V和自我发挥的赢家Z之间的误差，并最大限度地提高神经网络的转移概率的相似性搜索的概率π。具体来说，参数的调整θ梯度下降的损失函数L分别是均方误差和交叉熵的损失，其中C是L2正则化参数控制体重水平（防止过拟合）。

<img src="/img/alphazero3.jpg" />

####alpha zero的经验训练

我们应用我们的强化学习管道来训练我们的程序alphago零。训练从完全随机的行为开始，持续3天没有人工干预。

在训练过程中，自490万场比赛共产生1600，使用模拟每个MCTS，相当于约每移动0.4s思考时间。参数从700000个小批量2048个位置更新。神经网络包含20个剩余块（见进一步细节的方法）。

alphago零自我发挥强化学习期间的表现，作为培训时间的函数，在ELO评分25。在整个训练过程中，学习进展顺利，并没有受到先前文献中所提出的振荡或灾难性遗忘的影响。令人惊讶的是，alphago零比AlphaGo Lee后仅36小时；相比之下，AlphaGo Lee被训练好几个月了。72个小时后，我们评估alphago零对AlphaGo Lee打败了Lee Sedol的版本，2小时的时间控制和匹配的条件下在汉城的人机匹配使用。alphago零使用单机4张量处理单元（TPU）而AlphaGo Lee分布在许多机器和使用48个。alphago零击败AlphaGo Lee以100比0（参见扩展数据图5和补充资料）。

评估自我发挥强化学习的优点，相对于从人体数据的学习，我们训练的第二神经网络（使用相同的架构）预测的数据集；取得了国家的最先进的预测精度比以前的工作。监督学习取得了更好的初始性能，并且更好地预测了人类职业游戏的结果。值得注意的是，虽然有监督的学习取得了较高的移动预测精度，自学的玩家表现得更好，总体而言，击败了受过训练的玩家在头24小时的培训。这表明，alphago零可以学习的策略，是对人类起到质的不同。



为了分离的结构和算法的贡献，我们用alphago零的神经网络体系结构的性能与以前的神经网络结构与AlphaGo Lee相比（见图4）。四个神经网络创建了，可以使用独立的政策和价值网络；用AlphaGo Lee或alphago零剩余网络架构的卷积网络架构。每个网络进行训练，最大限度地减少相同的损失函数（公式1）使用一个固定的数据集生成的alphago零自玩游戏72小时后自游戏训练。利用剩余网络更准确，达到较低的错误，和改进的性能在alphago超过600的Elo。结合政策和价值结合成一个单一的网络，降低移动的预测精度，但降低值误差和性能的提高起alphago大约600的Elo。这部分是由于提高了计算效率，但更重要的是双重目的规范网络共同表示支持多个用例。

<img src="/img/alphazero4.jpg" />

####alpha zero学到的知识

随后我们将我们的强化学习的管道，使用一个更大的神经网络的alphago零的第二个实例，在一个较长的时间。训练又从完全随机的行为开始，持续了大约40天。
在训练过程中，产生了2900万个自我游戏。参数从310万个小批次中更新，每个2048个位置。神经网络包含40个残差块。在整个训练过程中定期播放的游戏在扩展数据图4和补充信息中显示。

<img src="/img/alphazero5.jpg" />

图5：去alphago零知识。一个五人的定式（普通角序列）在alphago零发现训练。相关的时间戳显示第一次每个序列发生（以旋转和反射的帐户）自我发挥训练中。扩展数据图1为每个序列提供了超过训练的频率。B五定式的青睐在自我发挥不同训练阶段。每一个显示的角落序列在所有的角落序列中发挥最大的频率，在自我游戏训练的迭代中。该迭代的时间戳在时间轴上表示。在10小时内，弱角移动是首选。在47小时的3-3的入侵是最常播放的。这个定式人专业玩也很常见；然而alphago零后来发现和优选的新变化。扩展数据图2为所有五个序列和新的变化提供了随时间变化的频率。C三自玩游戏，在训练的不同阶段起着第一80个动作，使用1600个模拟（约0.4s）每搜索。在3小时内，游戏专注于捕捉石头，就像人类的初学者一样。在19小时内，游戏展现了生死、影响力和领土的基本面。在70小时内，这场比赛是完美的平衡，涉及多个战斗和复杂的KO战斗，最终解决了半分的胜利为白色。有关完整游戏的补充信息。


####结论
我们的研究结果全面证明，纯强化学习方法是完全可行的，即使是在最具挑战性的领域中：如果没有基本规则之外的领域知识，就有可能训练到超人级，没有人类实例或指导。此外，纯强化学习方法只需要训练几个小时，与人类专家数据训练相比，它取得了更好的渐近性能。使用这种方法，alphago零打败alphago最强的以前的版本，这是使用手工制作的特征的人体数据进行训练，大幅度。
人类从几千年来玩过的游戏中积累了大量的知识，并融为模式、谚语和书籍。在短短几天内，启动白板，alphago零能够重新发现许多这样的围棋知识，以及新的策略。










