<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bingjie Blog</title>
    <description>关于深度学习与量化交易、黑客与画家 | 高冰洁，Deep Learning &amp; Quantitive Analysis，Nature Language Processing，Machine Learning | 这里是 @Diane冰洁 的个人博客，与你一起发现更大的世界。</description>
    <link>http://0.0.0.0:4000/</link>
    <atom:link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 14 Jul 2017 16:59:17 +0800</pubDate>
    <lastBuildDate>Fri, 14 Jul 2017 16:59:17 +0800</lastBuildDate>
    <generator>Jekyll v3.5.0</generator>
    
      <item>
        <title>CNN学习总结(一)</title>
        <description>&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;#build&quot;&gt;这是一个简单而又痛苦的开始…&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;很久之前就学习了CNN但是一直没有做一个系统性的总结，最近开了博客就总结下。&lt;/p&gt;

&lt;p id=&quot;build&quot;&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文&lt;/h2&gt;
&lt;p&gt;首先阅读一段CNN代码：https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py。在此我截取一部分代码简述一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from __future__ import print_function

import tensorflow as tf

#导入Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&quot;/tmp/data/&quot;, one_hot=True)

#网络中的超参数
learning_rate = 0.001
training_iters = 200000
batch_size = 128
display_step = 10

#网络参数
n_input = 784 # MNIST data input (img shape: 28*28)
n_classes = 10 # MNIST total classes (0-9 digits)
dropout = 0.75 # Dropout, probability to keep units

#输入占位符
x = tf.placeholder(tf.float32, [None, n_input])
y = tf.placeholder(tf.float32, [None, n_classes])
keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)


#构造卷积层，其中SAME表示越过边缘VAILD代表不越过边缘

def conv2d(x, W, b, strides=1):
    # Conv2D wrapper, with bias and relu activation
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)


def maxpool2d(x, k=2):
    # MaxPool2D wrapper
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],
                          padding='SAME')


# Create model
def conv_net(x, weights, biases, dropout):
    # Reshape input picture
    x = tf.reshape(x, shape=[-1, 28, 28, 1])

    # Convolution Layer
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    # Max Pooling (down-sampling)
    conv1 = maxpool2d(conv1, k=2)

# Convolution Layer
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
# Max Pooling (down-sampling)
    conv2 = maxpool2d(conv2, k=2)

# Fully connected layer
# Reshape conv2 output to fit fully connected layer input
    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    # Apply Dropout
    fc1 = tf.nn.dropout(fc1, dropout)
# Output, class prediction
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out

# Store layers weight &amp;amp; bias
weights = {
    # 5x5 conv, 1 input, 32 outputs
    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),
    # 5x5 conv, 32 inputs, 64 outputs
    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),
    # fully connected, 7*7*64 inputs, 1024 outputs
    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),
    # 1024 inputs, 10 outputs (class prediction)
    'out': tf.Variable(tf.random_normal([1024, n_classes]))
}

biases = {
    'bc1': tf.Variable(tf.random_normal([32])),
    'bc2': tf.Variable(tf.random_normal([64])),
    'bd1': tf.Variable(tf.random_normal([1024])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}

# Construct model
pred = conv_net(x, weights, biases, keep_prob)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    while step * batch_size &amp;lt; training_iters:
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        # Run optimization op (backprop)
        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,
                                       keep_prob: dropout})
        if step % display_step == 0:
            # Calculate batch loss and accuracy
            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,
                                                              y: batch_y,
                                                              keep_prob: 1.})
            print(&quot;Iter &quot; + str(step*batch_size) + &quot;, Minibatch Loss= &quot; + \
                  &quot;{:.6f}&quot;.format(loss) + &quot;, Training Accuracy= &quot; + \
                  &quot;{:.5f}&quot;.format(acc))
        step += 1
    print(&quot;Optimization Finished!&quot;)

# Calculate accuracy for 256 mnist test images
    print(&quot;Testing Accuracy:&quot;, \
        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],
                                      y: mnist.test.labels[:256],
                                      keep_prob: 1.}))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;阅读完代码之后，我们来看一下什么是卷积，卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的，另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。&lt;/p&gt;

&lt;p&gt;回想一下BP神经网络。BP网络每一层节点是一个线性的一维排列状态，层与层的网络节点之间是全连接的。这样设想一下，如果BP网络中层与层之间的节点连接不再是全连接，而是局部连接的。这样，就是一种最简单的一维卷积网络。如果我们把上述这个思路扩展到二维，这就是我们在大多数参考资料上看到的卷积神经网络。&lt;/p&gt;

&lt;p&gt;全连接网络。如果我们有1000x1000像素的图像，有1百万个隐层神经元，每个隐层神经元都连接图像的每一个像素点，就有1000x1000x1000000=10^12个连接，也就是10^12个权值参数。&lt;p&gt;

&lt;p&gt;局部连接网络，每一个节点与上层节点同位置附件10x10的窗口相连接，则1百万个隐层神经元就只有100w乘以100，即10^8个参数。其权值连接个数比原来减少了四个数量级。&lt;/p&gt;

#CNN的结构

卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。 这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：
&lt;ul&gt;
&lt;li&gt; 特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取局部特征。一旦一个特征被提取出来，只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。
&lt;/li&gt;
&lt;li&gt;特征映射。网络的每一个计算层都是由多个特征映射组成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下共享相同的突触权值集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。
&lt;/li&gt;
&lt;li&gt;子抽样。每个卷积层后面跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他 形式的变形的敏感度下降的作用。
&lt;/li&gt;
&lt;/ul&gt;
卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。

&lt;img src=&quot;/img/cnn1.png&quot; /&gt;

&lt;p&gt;图：卷积神经网络的概念示范：输入图像通过和三个可训练的滤波器和可加偏置进行卷积，卷积后在C1层产生三个特征映射图，然后特征映射图中每组的四个像素再进行求和，加权值，加偏置，通过一个Sigmoid函数得到三个S2层的特征映射图。这些映射图再进过滤波得到C3层。这个层级结构再和S2一样产生S4。最终，这些像素值被光栅化，并连接成一个向量输入到传统的神经网络，得到输出。&lt;/p&gt;

&lt;p&gt;一般地，C层为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系也随之确定下来；S层是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。&lt;/p&gt;

&lt;p&gt;此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层（C-层）都紧跟着一个用来求局部平均与二次提取的计算层（S-层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。&lt;/p&gt;

#稀疏连接(Sparse Connectivity)

&lt;p&gt;卷积网络通过在相邻两层之间强制使用局部连接模式来利用图像的空间局部特性，在第m层的隐层单元只与第m-1层的输入单元的局部区域有连接，第m-1层的这些局部区域被称为空间连续的接受域。我们可以将这种结构描述如下：设第m-1层为视网膜输入层，第m层的接受域的宽度为3，也就是说该层的每个单元与且仅与输入层的3个相邻的神经元相连，第m层与第m+1层具有类似的链接规则，如下图所示。&lt;/p&gt;

&lt;img src=&quot;/img/cnn2.png&quot; width=&quot;300&quot; /&gt;

&lt;p&gt;可以看到m+1层的神经元相对于第m层的接受域的宽度也为3，但相对于输入层的接受域为5，这种结构将学习到的过滤器（对应于输入信号中被最大激活的单元）限制在局部空间模式（因为每个单元对它接受域外的variation不做反应）。从上图也可以看出，多个这样的层堆叠起来后，会使得过滤器（不再是线性的）逐渐成为全局的（也就是覆盖到了更大的视觉区域）。例如上图中第m+1层的神经元可以对宽度为5的输入进行一个非线性的特征编码。&lt;/p&gt;

#权值共享(Shared Weights)

在卷积网络中，每个稀疏过滤器hi通过共享权值都会覆盖整个可视域，这些共享权值的单元构成一个特征映射，如下图所示。

&lt;img src=&quot;/img/cnn3.png&quot; width=&quot;300&quot; /&gt;

&lt;p&gt;在图中，有3个隐层单元，他们属于同一个特征映射。同种颜色的链接的权值是相同的，我们仍然可以使用梯度下降的方法来学习这些权值，只需要对原始算法做一些小的改动，这里共享权值的梯度是所有共享参数的梯度的总和。我们不禁会问为什么要权重共享呢？一方面，重复单元能够对特征进行识别，而不考虑它在可视域中的位置。另一方面，权值共享使得我们能更有效的进行特征抽取，因为它极大的减少了需要学习的自由变量的个数。通过控制模型的规模，卷积网络对视觉问题可以具有很好的泛化能力。&lt;/p&gt;

#举例讲解：   

上面聊到，好像CNN一个牛逼的地方就在于通过感受野和权值共享减少了神经网络需要训练的参数的个数。那究竟是啥的呢？

下图左：如果我们有1000x1000像素的图像，有1百万个隐层神经元，那么他们全连接的话（每个隐层神经元都连接图像的每一个像素点），就有1000x1000x1000000=10^12个连接，也就是10^12个权值参数。然而图像的空间联系是局部的，就像人是通过一个局部的感受野去感受外界图像一样，每一个神经元都不需要对全局图像做感受，每个神经元只感受局部的图像区域，然后在更高层，将这些感受不同局部的神经元综合起来就可以得到全局的信息了。这样，我们就可以减少连接的数目，也就是减少神经网络需要训练的权值参数的个数了。如下图右：假如局部感受野是10x10，隐层每个感受野只需要和这10x10的局部图像相连接，所以1百万个隐层神经元就只有一亿个连接，即10^8个参数。比原来减少了四个0（数量级），这样训练起来就没那么费力了，但还是感觉很多的啊，那还有啥办法没？

&lt;img src=&quot;/img/cnn4.png&quot; /&gt;

&lt;p&gt;我们知道，隐含层的每一个神经元都连接10x10个图像区域，也就是说每一个神经元存在10x10=100个连接权值参数。那如果我们每个神经元这100个参数是相同的呢？也就是说每个神经元用的是同一个卷积核去卷积图像。这样我们就只有多少个参数？？只有100个参数啊！！！亲！不管你隐层的神经元个数有多少，两层间的连接我只有100个参数啊！亲！这就是权值共享啊！亲！这就是卷积神经网络的主打卖点啊！亲！（有点烦了，呵呵）也许你会问，这样做靠谱吗？为什么可行呢？这个……共同学习。&lt;/p&gt;

&lt;p&gt;假如一种滤波器，也就是一种卷积核就是提出图像的一种特征，例如某个方向的边缘。那么我们需要提取不同的特征，怎么办，加多几种滤波器不就行了吗？对了。所以假设我们加到100种滤波器，每种滤波器的参数不一样，表示它提出输入图像的不同特征，例如不同的边缘。这样每种滤波器去卷积图像就得到对图像的不同特征的放映，我们称之为FeatureMap。所以100种卷积核就有100个Feature Map。这100个FeatureMap就组成了一层神经元。到这个时候明了了吧。我们这一层有多少个参数了？100种卷积核x每种卷积核共享100个参数=100x100=10K，也就是1万个参数。才1万个参数啊！见下图右：不同的颜色表达不同的滤波器。
&lt;/p&gt;
&lt;img src=&quot;/img/cnn5.png&quot; /&gt;

 &lt;p&gt;遗漏一个问题。刚才说隐层的参数个数和隐层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关。那么隐层的神经元个数怎么确定呢？它和原图像，也就是输入的大小（神经元个数）、滤波器的大小和滤波器在图像中的滑动步长都有关！例如，我的图像是1000x1000像素，而滤波器大小是10x10，假设滤波器没有重叠，也就是步长为10，这样隐层的神经元个数就是(1000x1000 )/ (10x10)=100x100个神经元了，假设步长是8，也就是卷积核会重叠两个像素，那么……我就不算了，思想懂了就好。注意了，这只是一种滤波器，也就是一个FeatureMap的神经元个数哦，如果100个Feature Map就是100倍了。由此可见，图像越大，神经元个数和需要训练的权值参数个数的贫富差距就越大。
 &lt;/p&gt;
 &lt;p&gt;
 需要注意的一点是，上面的讨论都没有考虑每个神经元的偏置部分。所以权值个数需要加1 。这个也是同一种滤波器共享的。
 总之，卷积网络的核心思想是将：局部感受野、权值共享（或者权值复制）以及时间或空间亚采样这三种结构思想结合起来获得了某种程度的位移、尺度、形变不变性。
 &lt;/p&gt;
#The Full Model

&lt;p&gt;卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。网络中包含一些简单元和复杂元，分别记为S-元和C-元。S-元聚合在一起组成S-面，S-面聚合在一起组成S-层，用Us表示。C-元、C-面和C-层(Us)之间存在类似的关系。网络的任一中间级由S-层与C-层串接而成，而输入级只含一层，它直接接受二维视觉模式，样本特征提取步骤已嵌入到卷积神经网络模型的互联结构中。
&lt;/p&gt;
&lt;p&gt;一般地，Us为特征提取层(子采样层)，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系 也随之确定下来；
&lt;/p&gt;
Uc是特征映射层(卷积层)，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用 影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层(S-层)都紧跟着一个 用来求局部平均与二次提取的计算层(C-层)，这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。
&lt;/p&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 14 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://0.0.0.0:4000/2017/07/14/cnn-1/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2017/07/14/cnn-1/</guid>
        
        
      </item>
    
      <item>
        <title>Hello 2017</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;“Yeah It’s on. ”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;#build&quot;&gt;跳过废话，直接看技术实现 &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;作为一个程序员， Blog 这种轮子要是挂在大众博客程序上就太没意思了。一是觉得大部分 Blog 服务都太丑，二是觉得不能随便定制不好玩。之前因为太懒没有折腾，结果就一直连个写 Blog 的地儿都没有。&lt;/p&gt;

&lt;p id=&quot;build&quot;&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;正文&quot;&gt;正文&lt;/h2&gt;

&lt;p&gt;最近几年经过各种理财机构和专家的熏陶，普通人对于“资产配置”一词已经耳熟能详，甚至有点觉得是陈腔滥调了。但其实很多人不知道资产配置的基本原理和具体运用。其实这里包含了三部份：&lt;/p&gt;

&lt;p&gt;(1)市场分析&lt;/p&gt;

&lt;p&gt;(2)配置理论应用&lt;/p&gt;

&lt;p&gt;(3)再平衡方式（再平衡是指当资产组合因为市场波动而偏离了原有状态时，通过及时调整资产比例而实现组合的再平衡，耶鲁投资大师大卫.斯文森将资产配置再平衡形容为“天上掉下的馅饼”）&lt;/p&gt;

&lt;p&gt;而这三者形成了紧密的闭环, 就如同专业媒体常提到的耶鲁投资模式, 其实就是构建了一套完整的机构投资流程和不受市场情绪左右的严谨的投资原则，包括投资目的的设定、资金的进出、资产负债的配比、资产类别的划分及配置、投资品种和投资工具的选择、风险控制、基金经理的选择等。&lt;/p&gt;

&lt;p&gt;需要说明的是，在私人银行和财富管理界，资产配置是有严格要求的，必须以“大类资产配置”为基础，而不能以个股为基础。所谓大类资产，是指股票、债券、地产、黄金等“大类别”的资产，这些资产之间具有分散性，是资产配置的基础。&lt;/p&gt;

&lt;p&gt;无论资产配置的后台是人工还是电脑，这三部分都是必不可少的。但随着大数据技术和机器学习技术的普及，资产配置开始走向智能化，这里就结合我们的经验给大家做一些分享，谈谈机器学习是如何在资产配置中使用的。&lt;/p&gt;

&lt;p&gt;资产配置的理论选择&lt;/p&gt;

&lt;p&gt;模型开发者可根据市场不同情况，利用不同的配置理论与再平衡方式，为投资人提供智能配置与调仓服务, 我们举三种常见的配置组合类型如下：&lt;/p&gt;

&lt;p&gt;懒人组合(1/N):
在这种组合方式里，假设4个投资标的, 则每个配置25%,而懒人组合常搭配的调仓方式有三种, ：&lt;/p&gt;

&lt;p&gt;(a)买入并持有策略(Buy-and-hold Strategy)&lt;/p&gt;

&lt;p&gt;(b)恒定混合策略(Constant-mix Strategy)&lt;/p&gt;

&lt;p&gt;(c)固定比例投资组合保险策略 CPPI(Constant proportion portfolio insurance)&lt;/p&gt;

&lt;p&gt;我们以方法(b) 恒定混合策略为例, 看看当一定期间后该配比与原配置不同时, 触发调整进行再平衡是如何进行的。 首先, 假设我们配置在美国股市、中国股市、债市、黄金各25%,并设定一个季度后做出调整。&lt;/p&gt;

&lt;p&gt;那么在再平衡时, 美股、A股需要调降至25%, 而债市与黄金则需增配至25%, 尔后每季度调整, 即完成再平衡流程, 所以懒人投资法是不需要运用复杂的机器学习的方式。&lt;/p&gt;

&lt;p&gt;风险平价组合(Risk Parity):&lt;/p&gt;

&lt;p&gt;风险平价是对投资组合中不同资产分配相同的风险权重的一种资产配置理念, 在一般情况下股票、商品投资的风险较高, 债券的风险较低, 因此在配置时则会降低高风险资产配置, 使其所贡献的风险相同, 在假设资产相关性相等的条件下, 我们能把某一类资产i 藉由risk-parity计算后, 其配置权重表示如下, 但由于Risk-Parity 并不考虑收益, 只考量波动率(风险)，在实务应用中比较适合能提供良好收益的资产或产品, 因此普遍应用在Fund of funds (FOF)的配置模式, 当大家看完公式后, 是不是觉得你也能成为FOF 投资经理?&lt;/p&gt;

&lt;p&gt;但倘若要成为优秀的投资经理, 就必须对波动率(风险)衡量做番苦工, 传统的方式包括历史波动率模型(Exponential Weighted Moving Average，EWMA)、 隐含波动率模型（Implied Volatility）、以及时间序列一系列模型(GARCH)。&lt;/p&gt;

&lt;p&gt;随着机器学习的崛起,近期也发展出了基于大数据的深度学习模型来预测波动率。比如来自斯坦福大学和Google的学者联合发表了论文“Deep Learning Stock Volatilities with Google Domestic Trends”，利用Google搜索趋势的数据和递归神经元的学习方式来预测股市波动率，其预测效果比传统线性模型或GARCH模型提升31%以上。 因此只要运用得当, 对于采用Risk-Parity的方式, 能提供更好的配置结果。&lt;/p&gt;

&lt;p&gt;马科维茨的均值方差-有效前沿组合或Black-Litterment模型修正:&lt;/p&gt;

&lt;p&gt;有效前沿组合是多数学过金融学的学生都耳熟能详的配置方式, 在有n种资产的投资&lt;/p&gt;

&lt;p&gt;组合中，为各资产的投资权重，对应的收益率为，&lt;/p&gt;

&lt;p&gt;则投资组合的预期收益与险如下：&lt;/p&gt;

&lt;p&gt;模型主要寻找 “收益/风险”(单位风险下的收益)的优化配置组合, 因此优化方程&lt;/p&gt;

&lt;p&gt;表达式如下：&lt;/p&gt;

&lt;p&gt;经由拉格朗日乘值法求解得到的投资权重就是模型的最优投资方案。若把优化投资组合在以标准方差(波动率)为横坐标，预期收益率为纵坐标的二维平面中描绘出来，形成一条曲线。&lt;/p&gt;

&lt;p&gt;这条曲线越往右边投资组合风险越高, 但相对收益也较高, 也符合一般对于高风险对应高收益的认知, 既然有了这个特性, 我们便能将风险选择对应到不同客户属性的配置方案。因此有效前沿组合便成了资产配置的理论基础。&lt;/p&gt;

&lt;p&gt;构造资产配置组合的三大关键点&lt;/p&gt;

&lt;p&gt;现在我们确定用马科维茨的均值方差-有效前沿理论为资产配置组合的基础。&lt;/p&gt;

&lt;p&gt;从这我们可以衍生出三个构造模型的关键点：(1)如何预估风险、 (2)如何预估收益、(3)如何正确的分类用户属性 。&lt;/p&gt;

&lt;p&gt;（1）预估风险&lt;/p&gt;

&lt;p&gt;对于第1点的处理, 可以使用我们上文已经提过到过的，Deep Learning的方式来改善风险预估的困难程度。&lt;/p&gt;

&lt;p&gt;（2）预估收益&lt;/p&gt;

&lt;p&gt;而对于第2点收益预估部份, 运用机器学习好处有：输入资料形态限制较低；可作线性/非线性学习；自我演化、修正等等。机器学习可以对模型因子做出有监督学习, 因子的选择可包含基本面、技术面、筹码面数据, 并做出市场收益的对应估计。下图展示了美国股市运用决策树因子模型分析收益的案例。&lt;/p&gt;

&lt;p&gt;预估收益有多种办法。我们将几种不同的收益预估模型测算出来的资产收益，作为输入参数放入马科维茨的有效前沿模型或者Black-Litterment模型计算，并分析其对投资组合的影响，也就是投资收益的表现。&lt;/p&gt;

&lt;p&gt;下面我们用三种方法来预估收益，一种是支持向量机回归模型（SVM），一种是线性回归模型，这两种模型的分析因子包括利率、市盈率、市净率、股息率、企业盈收、成交值、隐含波动率、MACD、KD等。而第三种则是一般人最常用的，直接用过去几年市场的走势来预测未来市场。&lt;/p&gt;

&lt;p&gt;(3)客户属性分类&lt;/p&gt;

&lt;p&gt;最后我们来看第3点也就是客户分类的机器学习部份，坦率说迄今为止金融机构对用户的数据掌握最多，可以通过记录消费者的消费喜好、收入状况、年龄阶段，推荐客户可能需要的贷款、融资等金融产品。机器学习将用户数据收集后进行规整处理，转化为相同维度的特征向量，通过聚类，回归，关联等各种分类器。&lt;/p&gt;

&lt;p&gt;RFM模型是用户价值研究中的经典模型，基于近度 (Recency)、频度(Frequency)和额度(Monetary)这3个指标对用户进行聚类, 找出具有潜在价值的用户, 从而辅助商业决策，提高营销效率、复购率与转化率, 但多属于单一金融产品营销, 较适合于战术投资配置推荐, 对于风险承受匹配于马科维茨的配置应用则尚未成熟, 因此一般仍以符合监管的主动风险划分方式, 再依据模型优化解帮助客户提供投资资产组合。&lt;/p&gt;

&lt;p&gt;综上所述，我们不难发现机器学习对于资产配置组合的应用实践已经非常丰富, 对于金融机构与资产管理公司来说, 加强金融科技研发人才培育, 可能是未来几年的重要任务。&lt;/p&gt;

&lt;p&gt;参考文献：&lt;/p&gt;

&lt;p&gt;[1] S. Maillard, T. Roncalli and J. Teiletche: The properties of equally-weighted risk contributions portfolios. Journal of Portfolio Management,Vol.36, No.4 pp.60-70,2010.&lt;/p&gt;

&lt;p&gt;[2] R. Xiong, E.P.Nicholas and Y. Shen: Deep learning stock volatilities with google domestic trends. arXiv preprint arXiv:1512.04916,2015.&lt;/p&gt;

&lt;h2 id=&quot;后记&quot;&gt;后记&lt;/h2&gt;

&lt;p&gt;作者：贾宜宸
链接：https://www.zhihu.com/question/58616635/answer/160109081
来源：知乎
著作权归作者所有，转载请联系作者获得授权。&lt;/p&gt;

&lt;p&gt;—— Diane 后记于 2017.7&lt;/p&gt;

</description>
        <pubDate>Thu, 13 Jul 2017 20:00:00 +0800</pubDate>
        <link>http://0.0.0.0:4000/2017/07/13/hello-2015/</link>
        <guid isPermaLink="true">http://0.0.0.0:4000/2017/07/13/hello-2015/</guid>
        
        <category>生活</category>
        
        
      </item>
    
  </channel>
</rss>
